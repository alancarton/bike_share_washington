{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 3 - Bike Sharing Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Task description:\n",
    "\n",
    "* **Training data**: whole 2011 and first 3 quarters of 2012.\n",
    "* **Test data**: 4th quarter of 2012.  Do not fit your models with these data! They should just be used to see how good/bad your model predictions are.\n",
    "* **Error metric**: R2 score (scikit-learn's default for regression).\n",
    "* **Features to use**: at least the ones present in the data (except for cnt). Do not use both casual and registered columns, as cnt=casual+registered (you may use one, but not both). Additionally, you can use other sources of data you deem appropriate to predict from extra features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exploratory Data Analysis (descriptive analytics) (4 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The first step of the analysis is to get familiar with the data. After importing the datasets we perform some analysis to have a general idea about the data. \n",
    "* **.head**: to understand the columns of the datasets\n",
    "* **.describe**: to understand the distribution of the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In order to predict the feature cnt we need to delete casual and registered (being these variable sub classes of cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading bike-sharing-dataset.zip to C:\\Users\\ajukg\\Documents\\Python\\Assignments\n",
      "\n",
      "Bike-Sharing-Dataset.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0.00/273k [00:00<?, ?B/s]\n",
      "100%|##########| 273k/273k [00:00<00:00, 13.2MB/s]\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import requests \n",
    "import io \n",
    "import re\n",
    "\n",
    "#!kaggle datasets list -s bike-sharing-dataset\n",
    "!kaggle datasets download -d marklvl/bike-sharing-dataset --force\n",
    "\n",
    "from io import BytesIO\n",
    "zfname = 'bike-sharing-dataset.zip'\n",
    "\n",
    "with zipfile.ZipFile(zfname) as zfile:\n",
    "    for name in zfile.namelist():\n",
    "        print(name)\n",
    "        if re.search(r'\\.zip$', name) is not None:\n",
    "            # We have a zip within a zip\n",
    "            zfiledata = BytesIO(zfile.read(name))\n",
    "            with zipfile.ZipFile(zfiledata) as zfile2:\n",
    "                day = pd.read_csv(zfile2.open(\"day.csv\"))\n",
    "                hour = pd.read_csv(zfile2.open(\"hour.csv\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# importing the datasets\n",
    "# day = pd.read_csv(\"day.csv\")\n",
    "# hour = pd.read_csv(\"hour.csv\")\n",
    "del hour[\"casual\"]\n",
    "del hour[\"registered\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instant</th>\n",
       "      <th>dteday</th>\n",
       "      <th>season</th>\n",
       "      <th>yr</th>\n",
       "      <th>mnth</th>\n",
       "      <th>holiday</th>\n",
       "      <th>weekday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>weathersit</th>\n",
       "      <th>temp</th>\n",
       "      <th>atemp</th>\n",
       "      <th>hum</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>casual</th>\n",
       "      <th>registered</th>\n",
       "      <th>cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.344167</td>\n",
       "      <td>0.363625</td>\n",
       "      <td>0.805833</td>\n",
       "      <td>0.160446</td>\n",
       "      <td>331</td>\n",
       "      <td>654</td>\n",
       "      <td>985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2011-01-02</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.363478</td>\n",
       "      <td>0.353739</td>\n",
       "      <td>0.696087</td>\n",
       "      <td>0.248539</td>\n",
       "      <td>131</td>\n",
       "      <td>670</td>\n",
       "      <td>801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2011-01-03</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.196364</td>\n",
       "      <td>0.189405</td>\n",
       "      <td>0.437273</td>\n",
       "      <td>0.248309</td>\n",
       "      <td>120</td>\n",
       "      <td>1229</td>\n",
       "      <td>1349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2011-01-04</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.212122</td>\n",
       "      <td>0.590435</td>\n",
       "      <td>0.160296</td>\n",
       "      <td>108</td>\n",
       "      <td>1454</td>\n",
       "      <td>1562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2011-01-05</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.226957</td>\n",
       "      <td>0.229270</td>\n",
       "      <td>0.436957</td>\n",
       "      <td>0.186900</td>\n",
       "      <td>82</td>\n",
       "      <td>1518</td>\n",
       "      <td>1600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   instant      dteday  season  yr  mnth  holiday  weekday  workingday  \\\n",
       "0        1  2011-01-01       1   0     1        0        6           0   \n",
       "1        2  2011-01-02       1   0     1        0        0           0   \n",
       "2        3  2011-01-03       1   0     1        0        1           1   \n",
       "3        4  2011-01-04       1   0     1        0        2           1   \n",
       "4        5  2011-01-05       1   0     1        0        3           1   \n",
       "\n",
       "   weathersit      temp     atemp       hum  windspeed  casual  registered  \\\n",
       "0           2  0.344167  0.363625  0.805833   0.160446     331         654   \n",
       "1           2  0.363478  0.353739  0.696087   0.248539     131         670   \n",
       "2           1  0.196364  0.189405  0.437273   0.248309     120        1229   \n",
       "3           1  0.200000  0.212122  0.590435   0.160296     108        1454   \n",
       "4           1  0.226957  0.229270  0.436957   0.186900      82        1518   \n",
       "\n",
       "    cnt  \n",
       "0   985  \n",
       "1   801  \n",
       "2  1349  \n",
       "3  1562  \n",
       "4  1600  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "day.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "hour.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "hour.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "hour['cnt'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use(\"ggplot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# here we plot the hourly distribution of the bike ride count by weekdays, to understand daily usage patterns\n",
    "plt.figure(figsize=(14, 4))\n",
    "weekday_hr_gb = hour.groupby([\"weekday\", \"hr\"]).mean()[\"cnt\"].reset_index(drop=False)\n",
    "sns.pointplot(x=\"hr\", y=\"cnt\", hue=\"weekday\", data=weekday_hr_gb)\n",
    "plt.legend(loc=\"center left\", bbox_to_anchor=(1, 0.5))\n",
    "plt.title(\"hourly distribution, split by weekdays\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We notice the following: \n",
    "- weekdays - peaks are mornings and evenings\n",
    "- weekends - peaks are during the day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "weathersit_gb = hour.groupby([\"weathersit\", \"yr\"]).mean()[\"cnt\"].reset_index(drop=False)\n",
    "weathersit_gb[\"yr\"].replace([0, 1], [2011, 2012], inplace=True)\n",
    "sns.pointplot(hue=\"yr\", x=\"weathersit\", y=\"cnt\", data=weathersit_gb)\n",
    "plt.title(\"avg hourly count of bikes for every weather situation\")\n",
    "plt.xticks(\n",
    "    range(4),\n",
    "    [\"clear\", \"cloudy\", \"light \\nprecipitation\", \"heavy \\nprecipitation\"],\n",
    "    rotation=0,\n",
    "    fontsize=\"12\",\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "hum_gb = hour.groupby([\"hum\", \"yr\"]).mean()[\"cnt\"].reset_index(drop=False)\n",
    "hum_gb[\"yr\"].replace([0, 1], [2011, 2012], inplace=True)\n",
    "sns.lineplot(\"hum\", \"cnt\", hue=\"yr\", data=hum_gb, palette=\"Set1_r\")\n",
    "# sb.lineplot('hum','cnt',data=hum_gb[hum_gb['yr']==2012])\n",
    "plt.title(\"avg hourly count of bikes for humidity levels\")\n",
    "plt.legend([2011, 2012])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We have reasons to suspect correlation. The more humid it is, the less bikes are being rented out in 2011 the trend looks a bit different - where humidity is very low also less bikes are being used together with year it has some connection to our target value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "windspeed_gb = hour.groupby([\"windspeed\", \"yr\"]).mean()[\"cnt\"].reset_index(drop=False)\n",
    "windspeed_gb[\"yr\"].replace([0, 1], [2011, 2012], inplace=True)\n",
    "sns.lineplot(\"windspeed\", \"cnt\", hue=\"yr\", data=windspeed_gb, palette=\"Set1_r\")\n",
    "plt.title(\"avg hourly count of bikes for windspeed\")\n",
    "plt.legend([2011, 2012])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "As expected, there is no significant trend here to be seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "atemp_gb = hour.groupby([\"atemp\", \"yr\"]).mean()[\"cnt\"].reset_index(drop=False)\n",
    "atemp_gb[\"yr\"].replace([0, 1], [2011, 2012], inplace=True)\n",
    "sns.lineplot(hue=\"yr\", x=\"atemp\", y=\"cnt\", data=atemp_gb, palette=\"Set1_r\")\n",
    "plt.title('avg hourly count of bikes for different \"feels like\" temperatures')\n",
    "plt.legend([2011, 2012])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We see that the warmer it feels, the more bikes are being used but only up to a certain temperature. When it starts feeling very warm, less bikes are being rented out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# here we will plot the count of rides per month, while also representing its distribution\n",
    "ax = sns.boxplot(x=\"mnth\", y=\"cnt\", data=hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Here we plot the correlations among variables through a heatmap, which will be useful during the feature engineering phase.\n",
    "corrMatt = hour[[\"temp\", \"atemp\", \"hum\", \"windspeed\", \"cnt\"]].corr()\n",
    "\n",
    "mask = np.array(corrMatt)\n",
    "# Turning the lower-triangle of the array to false\n",
    "mask[np.tril_indices_from(mask)] = False\n",
    "fig, ax = plt.subplots()\n",
    "sns.heatmap(corrMatt, mask=mask, vmax=0.8, square=True, annot=True, ax=ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data Cleaning\n",
    "Here we star with cleaning the data. \n",
    "* **Step 1:** We split the variables into categorical and numerical to understand what variable can undergo what transformation\n",
    "* **Step 2:** We check for NAs\n",
    "* **Step 3:** We plot the different variables as boxplots to get a sense of the outlier distribution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Features class "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In order to improve the quality of the predictions, it is possible to transform wrongly encoded numerical feature in categorical ones. In this case \"hr\", \"weekday\", \"mnth\", \"season\", \"weathersit\", \"holiday\", \"workingday\" are encoded as numerical feature but they are clearly categorical. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#here we split the variables into categorical and numerical under two different lists\n",
    "from numpy import math\n",
    "import numpy as np\n",
    "\n",
    "catfeats = pd.DataFrame(hour.describe(include=[\"O\"])).columns\n",
    "numfeats = list(hour.select_dtypes(include=[np.number]).columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(catfeats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(numfeats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#here we coerce the variable type to categorical for these features\n",
    "categoryVariableList = [\n",
    "    \"hr\",\n",
    "    \"weekday\",\n",
    "    \"mnth\",\n",
    "    \"season\",\n",
    "    \"weathersit\",\n",
    "    \"holiday\",\n",
    "    \"workingday\",\n",
    "]\n",
    "for var in categoryVariableList:\n",
    "    hour[var] = hour[var].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "hour.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### NAs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "As it is possible to notice there are not NAs in the dataset. Therefore, no further action is required. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#here we go hunting for NAs\n",
    "for column in hour:\n",
    "    NAs = hour[column].isnull().sum()\n",
    "    print(column + \" \" + str(NAs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Outliers \n",
    "Here we have a look on the outliers. We will however not exclude any, as after iterating, we understood that we get better results withouth doing that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def show_outlier():\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=2)\n",
    "    fig.set_size_inches(12, 10)\n",
    "    sn.boxplot(data=hour, y=\"cnt\", orient=\"v\", ax=axes[0][0])\n",
    "    sn.boxplot(data=hour, y=\"cnt\", x=\"season\", orient=\"v\", ax=axes[0][1])\n",
    "    sn.boxplot(data=hour, y=\"cnt\", x=\"hr\", orient=\"v\", ax=axes[1][0])\n",
    "    sn.boxplot(data=hour, y=\"cnt\", x=\"workingday\", orient=\"v\", ax=axes[1][1])\n",
    "\n",
    "    axes[0][0].set(ylabel=\"Count\", title=\"Box Plot On Count\")\n",
    "    axes[0][1].set(xlabel=\"Season\", ylabel=\"Count\", title=\"Box Plot On Count Across Season\")\n",
    "    axes[1][0].set(\n",
    "        xlabel=\"Hour Of The Day\",\n",
    "        ylabel=\"Count\",\n",
    "        title=\"Box Plot On Count Across Hour Of The Day\",\n",
    "    )\n",
    "    axes[1][1].set(\n",
    "        xlabel=\"Working Day\", ylabel=\"Count\", title=\"Box Plot On Count Across Working Day\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "show_outlier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Feature Engineering\n",
    "Here we switch the original columns, with some dummy ones, so to fit the format needs of our algorithms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def generate_dummies(df, dummy_column):\n",
    "    dummies = pd.get_dummies(df[dummy_column], prefix=dummy_column)\n",
    "    df = pd.concat([df, dummies], axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "hour_m = pd.DataFrame.copy(hour)\n",
    "dummy_columns = [\"season\", \"yr\", \"mnth\", \"hr\", \"weekday\", \"weathersit\"]\n",
    "for dummy_column in dummy_columns:\n",
    "    hour_m = generate_dummies(hour_m, dummy_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# remove the original categorical variables: \"season\", \"yr\", \"mnth\", \"hr\", \"weekday\", \"weathersit\"\n",
    "for dummy_column in dummy_columns:\n",
    "    del hour_m[dummy_column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## drop also the variables 'instant' since it is irrelevant\n",
    "del hour_m['instant']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "hour_m.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Machine Learning (predictive analytics) (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In this part, we start applying the different models we want to apply. The order of this subchapter is the following:\n",
    "* **Step 1: Train and test split**\n",
    "* **Step 2: Regression model**\n",
    "* **Step 3: Random forest**\n",
    "* **Step 4: XGBoost**\n",
    "* **Step 5: Combining models**\n",
    "* **Step 6: Pipeline**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Training test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# here we create the training set from the complete dataset we have, using the date as filtering item\n",
    "X_train = hour_m.loc[hour_m[\"dteday\"] < \"2012-10-1\"]\n",
    "del X_train[\"cnt\"]\n",
    "del X_train[\"dteday\"]\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# here we create the test set from the complete dataset we have, using the date as filtering item\n",
    "X_test = hour_m.loc[hour_m[\"dteday\"] >= \"2012-10-1\"]\n",
    "del X_test[\"cnt\"]\n",
    "del X_test[\"dteday\"]\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "y_train = hour_m.loc[hour_m[\"dteday\"] < \"2012-10-1\"][\"cnt\"]\n",
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "y_test = hour_m.loc[hour_m[\"dteday\"] >= \"2012-10-1\"][\"cnt\"]\n",
    "y_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Regression Model\n",
    "Here we run our regression model, which is the first model we want to try as if with such a simple one we get good results, we would prefer it over more complex ones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Create linear regression object\n",
    "regr = linear_model.LinearRegression()\n",
    "\n",
    "# Train the model using the training sets\n",
    "regr.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions using the testing set\n",
    "y_pred_rm = regr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# here we calculate the r^2 for the model\n",
    "r2_rm = r2_score(y_test, y_pred_rm)\n",
    "r2_rm.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "accuracy_rm = cross_val_score(estimator=regr, X=X_train, y=y_train, cv=5)\n",
    "accuracy_rm[1].round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Lets compare the distribution of train and test results. the distribution of train and test looks very different. It confirms visually that our model has not predicted really in a proper way cnt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(ncols=2)\n",
    "fig.set_size_inches(12, 5)\n",
    "sn.distplot(y_test, ax=ax1, bins=50)\n",
    "sn.distplot((y_pred_rm), ax=ax2, bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(y_test, y_test, \"r--\", y_test, y_pred_rm, \"b,\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Random forest\n",
    "Here we run out random forest model. Also, using grid_search, we find the best parameters for this model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Grid Search\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "regressor = RandomForestRegressor()\n",
    "parameters = [\n",
    "    {\"n_estimators\": [150, 200, 250, 300], \"max_features\": [\"auto\", \"sqrt\", \"log2\"]}\n",
    "]\n",
    "grid_search = GridSearchCV(estimator=regressor, param_grid=parameters)\n",
    "grid_search = grid_search.fit(X_train, y_train)\n",
    "best_parameters = grid_search.best_params_\n",
    "best_accuracy = grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "best_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Random Forest Regression model\n",
    "# Use the best parameters found from above to build the model\n",
    "\n",
    "regressor = RandomForestRegressor(n_estimators=200, max_features=\"auto\")\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the values\n",
    "y_pred_rf = regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "r2_rf = r2_score(y_test, y_pred_rf)\n",
    "r2_rf.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Using k-fold cross validation to evaluate the performance of the model\n",
    "accuracy_rf = cross_val_score(estimator=regressor, X=X_train, y=y_train, cv=5)\n",
    "accuracy_rf = accuracy_rf.mean()\n",
    "accuracy_rf.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here we plot the variable importance to get a grasp of what the most important ones are, and the relative importance of each one as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Relative importance of features\n",
    "feature_importance = regressor.feature_importances_\n",
    "feature_importance = 100.0 * (feature_importance / feature_importance.max())\n",
    "sorted_idx = np.argsort(feature_importance)\n",
    "pos = np.arange(sorted_idx.shape[0]) + 0.5\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.barh(pos, feature_importance[sorted_idx], align=\"center\")\n",
    "plt.yticks(pos, X_train.columns[sorted_idx])\n",
    "plt.xlabel(\"Relative Importance\")\n",
    "plt.title(\"Variable Importance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(ncols=2)\n",
    "fig.set_size_inches(12, 5)\n",
    "sn.distplot(y_test, ax=ax1, bins=50)\n",
    "sn.distplot((y_pred_rf), ax=ax2, bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(y_test, y_test, 'r--', y_test, y_pred_rf, 'b,')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### XGBoost\n",
    "Here we run the XGBoost model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# here we coerce the variable type to 'int' to let XGBoost run\n",
    "X_train[\"holiday\"] = X_train.holiday.astype(\"int\")\n",
    "X_train[\"workingday\"] = X_train.workingday.astype(\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# here we coerce the variable type to 'int' to let XGBoost run\n",
    "X_test[\"holiday\"] = X_test.holiday.astype(\"int\")\n",
    "X_test[\"workingday\"] = X_test.workingday.astype(\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "xgb_model = xgb.XGBRegressor(\n",
    "    objective=\"reg:linear\",\n",
    "    random_state=42,\n",
    "    colsample_bytree=0.3,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=5,\n",
    "    alpha=10,\n",
    ")\n",
    "xgb_model.fit(X_train, y_train)\n",
    "y_pred_xgb_standard = xgb_model.predict(X_test)\n",
    "plt.scatter(y_test, y_pred_xgb_standard)\n",
    "r2_xgb_standard = r2_score(y_test, y_pred_xgb_standard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "accuracy_xgb_model = cross_val_score(estimator=xgb_model, X=X_train, y=y_train, cv=5)\n",
    "accuracy_xgb_model = accuracy_xgb_model.mean()\n",
    "accuracy_xgb_model.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(y_test, y_test, 'r--', y_test, y_pred_xgb_standard, 'b,')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# tuning Hyperparameters\n",
    "parameters_xgb = [\n",
    "    {\n",
    "        \"n_estimators\": [500, 1000],\n",
    "        \"max_depth\": [4, 8, 12],\n",
    "        \"colsample_bytree\": [0.4, 0.8],\n",
    "    }\n",
    "]\n",
    "grid_search = GridSearchCV(estimator=xgb_model, param_grid=parameters_xgb)\n",
    "grid_search = grid_search.fit(X_train, y_train)\n",
    "best_parameters_xgb = grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "best_parameters_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# finetuned_model\n",
    "model = xgb.XGBRegressor(max_depth=4, n_estimators=1000, colsample_bytree=0.4, seed=42)\n",
    "\n",
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    eval_metric=\"rmse\",\n",
    "    eval_set=[(X_train, y_train), (X_test, y_test)],\n",
    "    verbose=True,\n",
    "    early_stopping_rounds=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# here we do the actual prediction using\n",
    "y_pred_xgb = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "r2_xgb = r2_score(y_test, y_pred_xgb)\n",
    "r2_xgb.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "accuracy_xgb = cross_val_score(estimator=xgb_model, X=X_train, y=y_train, cv=5)\n",
    "accuracy_xgb = accuracy_xgb.mean()\n",
    "accuracy_xgb.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Relative importance of features\n",
    "feature_importance = model.feature_importances_\n",
    "feature_importance = 100.0 * (feature_importance / feature_importance.max())\n",
    "sorted_idx = np.argsort(feature_importance)\n",
    "pos = np.arange(sorted_idx.shape[0]) + 0.5\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.barh(pos, feature_importance[sorted_idx], align=\"center\")\n",
    "plt.yticks(pos, X_train.columns[sorted_idx])\n",
    "plt.xlabel(\"Relative Importance\")\n",
    "plt.title(\"Variable Importance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# here we plot to estimate the goodness of our prediction\n",
    "fig, (ax1, ax2) = plt.subplots(ncols=2)\n",
    "fig.set_size_inches(12, 5)\n",
    "sn.distplot(y_test, ax=ax1, bins=50)\n",
    "sn.distplot((y_pred_xgb), ax=ax2, bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(y_test, y_test, 'r--', y_test, y_pred_xgb, 'b,')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Combining Models\n",
    "In this section we combine some models, to see if we manage to achieve better results than the ones of the model which alone achieves the best results. We use random forest, gradient boosting and extra trees. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import ensemble\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In order to make this happen given limited computational power, we resized the dataset. This code therefore serves to  improve the running time and testing the code quality rather than given actual predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X_train_sample = hour_m[0:688]  # one month as training data\n",
    "del X_train_sample[\"cnt\"]\n",
    "del X_train_sample[\"dteday\"]\n",
    "\n",
    "X_test_sample = hour_m.loc[688:829]  # one week to be predicted\n",
    "del X_test_sample[\"cnt\"]\n",
    "del X_test_sample[\"dteday\"]\n",
    "\n",
    "y_train_sample = hour_m[0:688][\"cnt\"]\n",
    "\n",
    "y_test_sample = hour_m[688:830][\"cnt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# here we define the models\n",
    "rf = ensemble.RandomForestClassifier()\n",
    "gbm = ensemble.GradientBoostingClassifier()\n",
    "et = ensemble.ExtraTreesClassifier()\n",
    "\n",
    "combo = ensemble.VotingClassifier(\n",
    "    estimators=[(\"rf\", rf), (\"gbm\", gbm), (\"et\", et)],\n",
    "    voting=\"soft\",\n",
    "    weights=[3, 5, 2],\n",
    "    n_jobs=10,\n",
    ")\n",
    "\n",
    "combo.fit(X_train_sample, y_train_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#here we do the prediction\n",
    "y_pred_combo = combo.predict(X_test_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# here we measure the r^2\n",
    "r2_combo = r2_score(y_test_sample, y_pred_combo)\n",
    "r2_combo.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# here we measure the accuracy\n",
    "accuracy_combo = cross_val_score(\n",
    "    estimator=combo, X=X_train_sample, y=y_train_sample, cv=5\n",
    ")\n",
    "accuracy_combo = accuracy_combo.mean()\n",
    "accuracy_combo.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# here we plot our predictions' distribution vs. the actual distribution to estimate the goodness of the model.\n",
    "fig, (ax1, ax2) = plt.subplots(ncols=2)\n",
    "fig.set_size_inches(12, 5)\n",
    "sn.distplot(y_test_sample, ax=ax1, bins=50)\n",
    "sn.distplot((y_pred_combo), ax=ax2, bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(y_test_sample, y_test_sample, 'r--', y_test_sample, y_pred_combo, 'b,')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Pipeline\n",
    "Here we try to build a pipeline, where we figure out our best parameters for the logistic regression we run after it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "steps = [('scaler', StandardScaler()), ('SVM', SVC())]\n",
    "from sklearn.pipeline import Pipeline\n",
    "pipeline = Pipeline(steps) # define the pipeline object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "parameteres = {'SVM__C':[0.001,0.1,10,100,10e5], 'SVM__gamma':[0.1,0.01]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "grid = GridSearchCV(pipeline, param_grid=parameteres, cv=5)\n",
    "grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "X_train_sample[\"holiday\"] = X_train_sample.holiday.astype(\"int\")\n",
    "X_train_sample[\"workingday\"] = X_train_sample.workingday.astype(\"int\")\n",
    "X_test_sample[\"holiday\"] = X_test_sample.holiday.astype(\"int\")\n",
    "X_test_sample[\"workingday\"] = X_test_sample.workingday.astype(\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "grid.fit(X_train_sample, y_train_sample)\n",
    "print (grid.score(X_test_sample,y_test_sample))\n",
    "print (grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Logistic Regression - Showing the Classification Report.\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logmodel = LogisticRegression(\n",
    "    C=10,\n",
    "    class_weight=None,\n",
    "    dual=False,\n",
    "    fit_intercept=True,\n",
    "    intercept_scaling=1,\n",
    "    max_iter=200,\n",
    "    multi_class=\"ovr\",\n",
    "    n_jobs=1,\n",
    "    penalty=\"l2\",\n",
    "    random_state=None,\n",
    "    solver=\"liblinear\",\n",
    "    tol=0.0001,\n",
    "    verbose=0,\n",
    "    warm_start=False,\n",
    ")\n",
    "logmodel.fit(X_train_sample, y_train_sample)\n",
    "\n",
    "# Predicting on Test\n",
    "y_pred_pip_sample = logmodel.predict(X_test_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "r2_pip = r2_score(y_test_sample, y_pred_pip_sample)\n",
    "r2_pip.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "accuracy_pip = cross_val_score(estimator = logmodel, X = X_train_sample, y = y_train_sample, cv =5)\n",
    "accuracy_pip = accuracy_pip.mean()\n",
    "accuracy_pip.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here we plot the distribution of the predictions and the actual distribution to see how similar they are. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(ncols=2)\n",
    "fig.set_size_inches(12, 5)\n",
    "sn.distplot(y_test, ax=ax1, bins=50)\n",
    "sn.distplot((y_pred_pip_sample), ax=ax2, bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(y_test_sample, y_test_sample, 'r--', y_test_sample, y_pred_pip_sample, 'b,')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Wrap up\n",
    "To sum up, here is once again the steps we went through: \n",
    "* **Step 1**: Exploratory data analysis: here we explore and get familiar with the data (graphically and numerically) \n",
    "    -  We plot the explanatory variables with the target variable to understand their relations graphically\n",
    "* **Step 2**: Data cleaning: here we deal with outliers & NAs. \n",
    "   -  We change the class of certain variables to categorical to better fit our needs. Outliers did not need to be dropped as we get better results withouth this move. \n",
    "* **Step 3**: Feature engineering: we replace some of the columns with dummy columns in the right formats for our algorithms. \n",
    "* **Step 4**: Machine Learning: here we run different models individually, then we combine them and then we use a pipeline. \n",
    "    - Below a summary of the results of the various models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#here we summarize in a table the metrics of the various models we run to provide an overview\n",
    "data = [\n",
    "    {\n",
    "        \"regression\": r2_rm,\n",
    "        \"random_forest\": r2_rf,\n",
    "        \"xgb_standard\": r2_xgb_standard,\n",
    "        \"xgb_tuned\": r2_xgb,\n",
    "        \"combo*\": r2_combo,\n",
    "        \"pipeline*\": r2_pip,\n",
    "    },\n",
    "    {\n",
    "        \"regression\": accuracy_rm[1],\n",
    "        \"random_forest\": accuracy_rf,\n",
    "        \"xgb_standard\": accuracy_xgb_model,\n",
    "        \"xgb_tuned\": accuracy_xgb,\n",
    "        \"combo*\": accuracy_combo,\n",
    "        \"pipeline*\": accuracy_pip,\n",
    "    },\n",
    "]\n",
    "\n",
    "models_comparison = pd.DataFrame(\n",
    "    data,\n",
    "    index=[\"r2\", \"accuracy\"],\n",
    "    columns=[\n",
    "        \"regression\",\n",
    "        \"random_forest\",\n",
    "        \"xgb_standard\",\n",
    "        \"xgb_tuned\",\n",
    "        \"combo*\",\n",
    "        \"pipeline*\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "models_comparison.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# here we plot the predictions of the various models against the actual values, so to compare them again.\n",
    "\n",
    "def comparison_show():\n",
    "    fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\n",
    "\n",
    "    ax1 = axs[0]\n",
    "    index = [\"regression\", \"random_forest\", \"xgb_tuned\"]\n",
    "    r2 = models_comparison.loc[[\"r2\"], [\"regression\", \"random_forest\", \"xgb_tuned\"]].values[\n",
    "        0\n",
    "    ]\n",
    "    bar_width = 0.35\n",
    "    opacity = 0.8\n",
    "    ax1.bar(index, r2, bar_width, alpha=opacity, color=\"b\", label=\"r2\")\n",
    "\n",
    "    ax1.set_title(\"Scores by model\")\n",
    "    ax1.legend()\n",
    "\n",
    "    ax2 = axs[1]\n",
    "    index = [\"regression\", \"random_forest\", \"xgb_tuned\"]\n",
    "    accuracy = models_comparison.loc[\n",
    "        [\"accuracy\"], [\"regression\", \"random_forest\", \"xgb_tuned\"]\n",
    "    ].values[0]\n",
    "    bar_width = 0.35\n",
    "    opacity = 0.8\n",
    "    ax2.bar(index, accuracy, bar_width, alpha=opacity, color=\"r\", label=\"accuracy\")\n",
    "\n",
    "    ax2.set_title(\"Scores by model\")\n",
    "    ax2.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "comparison_show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As we can see, XGBoost tuned presents the best metrics by far. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# here we plot the predictions of the various models against the actual values, so to compare them again.\n",
    "\n",
    "def comparison_show_plots():\n",
    "\n",
    "    fig, axs = plt.subplots(nrows=2, ncols=2, figsize=(27, 22))\n",
    "\n",
    "    ax1 = axs[0, 0]\n",
    "    ax1.plot(y_test, y_test, \"r--\", y_test, y_pred_rm, \"b,\")\n",
    "    ax1.set_title(\"Regression Model\")\n",
    "\n",
    "    ax2 = axs[0, 1]\n",
    "    ax2.plot(y_test, y_test, \"r--\", y_test, y_pred_rf, \"b,\")\n",
    "    ax2.set_title(\"Random Forest\")\n",
    "\n",
    "    ax1 = axs[1, 0]\n",
    "    ax1.plot(y_test, y_test, \"r--\", y_test, y_pred_xgb_standard, \"b,\")\n",
    "    ax1.set_title(\"XG Boost Standard\")\n",
    "\n",
    "    ax2 = axs[1, 1]\n",
    "    ax2.plot(y_test, y_test, \"r--\", y_test, y_pred_xgb, \"b,\")\n",
    "    ax2.set_title(\"XG Boost Tuned\")\n",
    "\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "comparison_show_plots()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As we can clearly see, XGBoost after tuning reaches the best results. Compared to the other models, the performance is sensibly higher. The increase in the R^2 of the XGBoost, through hyperparameter tuning, was the turning point which allowed us to reach predictions which are this good. "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
